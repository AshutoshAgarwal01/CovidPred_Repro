%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper, varwidth=true, border=2pt]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{graphicx}
\usepackage{tabularx}
\graphicspath{ {./Artifact/} }
%\documentclass[varwidth=true, border=2pt]{standalone}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}

\usepackage[belowskip=-10pt,aboveskip=0pt]{caption}

\usepackage{xparse}
\NewDocumentCommand{\DIV}{om}{%
  \IfValueT{#1}{\setcounter{#2}{\numexpr#1-1\relax}}%
  \csname #2\endcsname
}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

% Content lightly modified from original work by Jesse Dodge and Noah Smith


\newcommand\BibTeX{B\textsc{ib}\TeX}
\title{CS598 DL4H Spring 2022 Project Draft}
\author{Ashutosh Agarwal and Chandan Goel \\
  \texttt{aa61@illinois.edu, chandan3@illinois.edu}
  \\[2em]
  Group ID: 179\\
  %% Paper ID: \href{https://www.hindawi.com/journals/ijbi/2020/8889023/}{External [7]}\\
  Paper ID: \href{https://www.hindawi.com/journals/ijbi/2020/8889023/}{External (Sharma, Rani \& Gupta 2020) [7]}\\
  Presentation link (TODO): \url{https://www.youtube.com} \\
  Code link: \url{https://github.com/AshutoshAgarwal01/CovidPred_Repro}} 

\begin{document}
\maketitle

% All sections are mandatory.
% Keep in mind that your page limit is 8, excluding references.
% For specific grading rubrics, please see the project instruction.

\section{Introduction}
This work \href{https://www.hindawi.com/journals/ijbi/2020/8889023/}{(Sharma, Rani \& Gupta 2020) [7]} is done to build efficient deep leaning models using the convolutional neural network (CNN) using a very limited set of chest X-ray images to differentiate COVID-19 cases from healthy cases and other types of illnesses. 

Several other studies were done before this work to classify COVID-19 patients using chest X-ray images. This study recognizes following issues in classification of COVID-19 based on chest X-ray that result in poor performance of models. 

\setlist{nolistsep}
\begin{itemize}[noitemsep]
\itemsep0em 
\item Scarcity of x-ray images available to train a viable deep learning model. 
\item Biased learning of the deep learning based model when images of multiple age groups are combined together to form a data set. E.g., x-ray images of pediatric patients combined with adult patients. 
\end{itemize}
 
In this study, authors have applied following techniques to overcome issues mentioned above: 

\begin{itemize}
\itemsep0em 
\item To overcome scarcity of x-ray images, authors proposed creating a much larger artificial dataset using smaller original dataset. This artificial dataset was created by applying 25 image augmentation techniques on the original data. 
\item Careful image selection: Authors propose to use similar type of images (same view, same age group) to train a very targeted model. This study claims that models built with similar type of images perform better than those models that take a wide variety of images. 
\end{itemize}

Overall, authors showed that artificially generated x-ray images using image augmentation techniques greatly improved model performance when compared with original smaller set of images. 

\section{Scope of reproducibility}

    \textit{"This study shows that a deep learning model trained with larger volume of artificially generated X-ray images using image augmentation techniques will have higher accuracy (TPR) than the model trained with small set of original images."}
    
\textit{Note: In this document we will abbreviate true positive rate a.k.a. Recall as TPR}

Data scarcity is a well-known problem in the field of healthcare analytics, especially when research is aimed at fairly new area e.g., classification of COVID-19 patients when the pandemic just broke out. We are motivated to take on this study as it tries to solve a real life problem which exists across all  healthcare domain. It will not only help improve model performance, but also reduce cost and time involved in getting sufficient size data for any healthcare study.  

\subsection{Addressed claims from the original paper}

We will be testing following two claims from the paper.
\begin{itemize}
    \item \textbf{Claim 1}: Model trained with original and augmented images combined results in higher accuracy (TPR) than model trained with only original images across all classification labels.
    
    Authors compared performance of following two models on unseen external data. 

    \begin{itemize}
        \item Model with only original images. 
        \item Model with original images along with augmented images.
    \end{itemize}
    
    Authors concluded that model trained with original and augmented images combined, results in higher accuracy (TPR) across classification labels.  In our project work, we want to confirm this claim.
    
    \item \textbf{Claim 2}: Authors showed that models trained with 120 and 140 degree rotated images were complimentary to each other. i.e., if one model performed bad for certain label than other model would perform better for the same label. Together, these two models beat performance of model trained with original images. We will verify this claim in our reproduction study.
\end{itemize}

\section{Methodology}
\begin{figure*}[t]
  \includegraphics[width=\textwidth]{acl-ijcnlp2021-templates/Artifacts/CNN Architecture_v2.png}
  \caption{Model architecture}
  \label{fig:model-arch}
\end{figure*}

We did this work in three phases, namely i) image pre-processing, (ii) data augmentation, (iii) training of deep learning models. The above steps are explained in this section. We leveraged the authors code (\href{https://github.com/arunsharma8osdd/covidpred}{GitHub [4]}) with moderate modifications for the model training but used our own code for data processing.

%%The authors of the original paper made a diligent effort for anyone to understand their paper and code easily. They provided link to their GitHub repository \href{https://github.com/arunsharma8osdd/covidpred}{CovidPred} [4] where code was present and they clearly mentioned source of data that they used for their study.

%%We are planning to write new code for data processing and re-using (with moderate modifications) existing code for model training and testing. Following sections provide more details about this.

\subsection{Model descriptions}
The author used a simple CNN approach by testing a number of different architectures with different number of layers, number of neurons per layers, normalizations, pooling techniques and hyperparameters and adapting the model after each additional improvement of validation accuracy (TPR). The CNN model identified uses 3 convolutional layers followed by two fully connected linear layers. 

\textbf{Figure \ref{fig:model-arch}} - manifests the CNN model architecture developed for this paper. Rectified Linear Unit (ReLU) addressed the non- linearity after each convolutional layer.

%% , and dropout between the connected linear layers were used to standardize the inputs to a neural network and stabilize the validation loss along the epochs, reducing the variance of validation accuracy. 

The first convolutional layer has an input channel of dimension 3 since we are using RGB image of size 256 x 256. The kernel size is chosen to be of size 3x3 with stride of 1. The output dimension at this layer is 32 x 256 x256. This is followed by ReLu activation function to introduce non-linearity. To reduce the number of training parameters and computation cost in conjunction with controlling overfitting issue, a max pooling layer with kernel size 2x2 and stride 2 has been introduced after ReLU activation function. Max pooling layer down sample the output feature maps to 32 x 128 x 128. The second convolutional layer has an input channel of 32 to keep it consistent with the dimension of the feature map from the previous layer. The kernel size is chosen to be 5x5 with stride of 1. The output dimension at this layer is 64 x 128 x 128 so no changes occurred in the transformation channel dimension. Like previous channel ReLU activation function is applied followed by another max pooling layer with kernel size 2*2 with stride 2 is introduced to down sample the feature map led to increased efficiency in model training process. The down-sampled feature map dimension: 64 x 64 x 64.

The third convolutional layer is introduced to upgrade the output channel before feeding into linear layers. Like previous convolutional layers, subsequent maxpool, ReLU activation are applied to the output images. The output dimension at this layer is 128 x 32 x 32. Finally, two fully connected linear layers are used at the end. A flattened version of the feature map is passed to the first fully connected layer. So, the input dimension size has become 128 x 32 x 32 = 131072
nodes. Then this layer is connected to the final linear layer. The output dimension of final layer should match the total image corresponds to five categories: Normal, TB, Pneumonia, Covid-19, Non Covid-19.

\subsection{Data descriptions}
Data from following three sources is used in this paper.

\begin{itemize}
    \item Github (Cohen's covid-chestxray-dataset) [1]: This dataset is used to get 'covid 19' and 'non covid 19' images 
    \item Kaggle NIH dataset [2]: This dataset is used to get Pneumonia images. 
    \item National Library of medicine [3]: This dataset is used to get normal and TB images. 
\end{itemize}

\textbf{Figure \ref{fig:data-processing}} depicts all steps we performed to gather and process data for model training and validation purposes.
\begin{figure}
\includegraphics[scale=0.45]{acl-ijcnlp2021-templates/Artifacts/Data_Processing.png}
\caption{Number in each box is count of images.}
\label{fig:data-processing}
\end{figure}
%% \textit {Note: Number in each box represents number of images in respective dataset.}

We filtered images from these datasets using following criteria: i). Age of patient must be 19 years or older, ii) only chest X-ray images and iii) image view must be PA

After filtering the data, we divided the dataset into two parts using random sampling. We reserved 10 percent of the data for external validation (test set) and remaining 90 percent for model training (training set).

After this, we created 25 new datasets by applying different augmentation techniques on original set of training and test images. Some of the augmentation techniques used are:
\begin{itemize}
    \item Rotate images by 45, 60, 90, 120, 140 and 160 degrees
    \item Raise blue, green, red and hue
    \item Crop images
    \item Flip images horizontally, vertically and in both directions.
    \item Introduce blur to the images.
\end{itemize}

We used CloDSA [5] library for image augmentation.

We further created one more dataset by combining original dataset and all augmented datasets. Thus we had total 27 datasets.

%% Following table shows distribution of images.
%% \begin{figure}
%% \includegraphics[scale=0.4]{acl-ijcnlp2021-templates/Artifacts/Data_Distribution.png}
%% \end{figure}

\subsection{Hyperparameters}

For initial reproduction work, we used same hyperparameters used in the paper except number of iterations. Table in \textbf{Figure \ref{fig:hyperparameters}} describes the hyperparameters and their values across different datasets. We used AdamOptimizer with learning rate of 1e-4.

\begin{figure}
\includegraphics[scale=0.61]{acl-ijcnlp2021-templates/Artifacts/Hyperparameters.png}
\caption{Hyperparameters}
\label{fig:hyperparameters}
\end{figure}

\subsection{Implementation}
We thoroughly studied the code available in author's git repository [4]. Following is distribution of code that we wrote and re-used.

\begin{itemize}
    \item Data processing: We wrote our own code from scratch for data processing. 
    \item Model training and validation: We made some improvements in existing code like logging, refactoring, minor performance improvements and training/ validating models using multiple datasets by using one script etc. But overall, it is heavily inspired by original work.
\end{itemize}

Code of reproduction work can be found in Git repository CovidPred\_Repo [6]
%% \url{https://github.com/AshutoshAgarwal01/CovidPred_Repro}

\subsection{Computational requirements}
In the proposal we mentioned that we will use GPU if training model with large dataset (all augmented images combined) takes very long time. However in our initial runs, even the largest dataset completed in reasonable amount of time. Therefore we decided to use the alternate in-premise CPU based hardware. Tables (\textbf{Figure \ref{fig:configuration}} and \textbf{Figure \ref{fig:resource-utilization}}) show configuration of desktop that we used and total resources consumed during work.
\begin{figure}
\includegraphics[scale=0.9]{acl-ijcnlp2021-templates/Artifacts/DesktopConfig.png}
\caption{Configuration of hardware}
\label{fig:configuration}
\end{figure}
\begin{figure}
\includegraphics[scale=0.8]{acl-ijcnlp2021-templates/Artifacts/ResourcesConsumed.png}
\caption{Resource utilization}
\label{fig:resource-utilization}
\end{figure}

Table in \textbf{Figure \ref{fig:cputime}} summarizes total CPU time and average CPU time per epoch for each model.
\begin{figure}
\includegraphics[scale=0.5]{acl-ijcnlp2021-templates/Artifacts/Training_Time.png}
\caption{CPU time spent for training}
\label{fig:cputime}
\end{figure}

%% Total size of original, filtered, augmented and combined data together was \textbf{119 GB} on disk.

\section{Results}
Our reproduction study did not result in exact same accuracy numbers as the original study. However overall trend of the accuracy (TPR) per label is consistent with the original study.

%%The results obtained by our reproduction study support both the claims cited in section 2 of this paper. We will describe them in following sections.

% The number of subsections for results should be the same as the number of hypotheses you are trying to verify.

\subsection{Result 1: (Results/ Analyses/ Plans)}

%\subsubsection{Result 1}


The paper claimed that model trained with original and augmented images combined results in higher accuracy (TPR) across all classification labels when compared with model trained with original images only.

%Table in \textit{figure \ref{fig:orig-comb-perf}} summarizes true positive rate (proportions of images correctly classified by the model for given label) of both models on unseen original data.

Table in \textbf{Figure \ref{fig:orig-comb-perf}} summarizes true positive rate (proportions of images correctly classified by the model for given label) of both models on unseen original data.
We can see that model trained with combined dataset outperforms model trained with original images by over 20\% on average. This upholds the paper's conclusion that overall it performs better than the baseline. 

We observed that the combined model performs 100\% better on Normal, over 42\% better on Covid-19, over 8\% better on Pneumonia and 25\% better on TB illnesses. However, accuracy (TPR) for non-covid degrades in combined model. As per our analysis, we feel it happened due to a smaller test dataset size of 3 images. 

We plan to introduce more non-covid19 images to our model in final study to confirm our analysis. In addtion, we also plan simulate the study multiple times to further analyze the outcome for a more accurate comparison.


\begin{figure}
\includegraphics[scale=0.67]{acl-ijcnlp2021-templates/Artifacts/Perf_comp_with_combined.png}
\caption{Original vs combined performance (true positive rate)}
\label{fig:orig-comb-perf}
\end{figure}

\subsection{Result 2: (Results/ Analyses/ Plans)}
Authors showed that performance of models trained with 120 and 140 degree rotated images were complimentary to each other. Together, these two models beat performance of model trained with original images. 

Table in \textbf{Figure \ref{fig:orig-aug-perf}} summarizes true positive rate (proportions of images correctly classified by the model for given label) for all three models on unseen original data.

We observed that the two models perform with 100\% accuracy (TPR) on Normal images. Model with 120 degree performs better in detecting Covid-19 while Model with 140 degree performs better on other illnesses. However, both augmented models perform worse than baseline for TB. Overall, the result trend with exception to TB is in line with the original study. Therefore, this claim is only partially supported by our reproduction study. 

We found that the difference is due to random sampling of TB images when compared with original study. We plan to further analyze and test the model with additional random sampling and compare the average result to study the trend. We also plan to simulate the study with new data sets for comparison.

%% As per our analysis we found that the difference is because of random sampling of TB images when compared with original study. We plan to further analyze and test the model with additional random sampling and compare the average result to study the trend.

\begin{figure}
\includegraphics[scale=0.7]{acl-ijcnlp2021-templates/Artifacts/Perf_comp_with_augmented.png}
\caption{Original vs rotated comparison (true positive rate)}
\label{fig:orig-aug-perf}
\end{figure}

\subsection{Additional results not present in the original paper}

We noticed that original authors used recall (true positive rate) as the metric to evaluate the models. However, we feel that in case of classification study, using recall alone is not appropriate to judge a model. 

Therefore, we decided to calculate accuracy, precision, recall and F1-score as well for these results. We tested model trained with original images and combined images against unseen original images. Table in \textbf{Figure \ref{fig:f1-score}} summarizes results of this comparison. We are showing only precision, recall and f1-score for conciseness.

In \textbf{Figure \ref{fig:f1-score}} recall for label non-covid19 using original model is 0.67 which is good, but at the same time precision (0.29) is very low which is bad. Thus, looking at recall alone is not sufficient. Therefore, we should use another stable metric e.g., f1-score to evaluate a model in a better way.

\begin{figure}
\includegraphics[scale=0.7]{acl-ijcnlp2021-templates/Artifacts/f1_score.PNG}
\caption{Original and combined validated against original unseen images}
\label{fig:f1-score}
\end{figure}

We can see that recall has significantly improved in combined model for all labels except non-covid which makes us feel that paper's approach of augmentation did not fully satisfy the expected results. However, F1-score has improved across all labels. This refutes the conclusion made by just observing recall and tells us that the combined model is actually better than original model.

By means of this ablation we can conclude that author's first claim is true and reproducible.

Original paper does not provide confusion matrix therefore, we cannot compare our f1-score against same metric obtained in original work.

\section{Discussion}
The claims cited in the original paper were largely reproducible in our work with couple of exceptions.  

\textbf{What was not reproducible: } The original paper claimed that the model trained with original and augmented images combined performed better than model trained with original images only. In our study, we observed that this claim is generally true with exception to non-covid class.  

Another claim that original paper made was that models trained with 120 degree rotated and 140 degree rotated images are complimentary to each other and combined, they perform better than model trained with original images. However we found that for TB class, both of these models together performed worse than the model with original images. 

We feel that very small dataset size for non-covid and TB classes led to these issues. There were only 3 and 5 images in validation set for non-covid and TB classes respectively. The small dataset for model validation caused too much variation in the results. As a result, we observed significant difference in accuracy (TPR) for these classes between original paper and our reproduction study.  

We feel that if we and original paper had larger dataset available for validation then this variation would have been smaller and we would have seen better accuracy (TPR) for predicting non-covid and TB classes respectively. 

\textbf{What was reproducible: } Overall although our accuracy (TPR) results are not exactly the same as those mentioned in the paper, however the trend was same. This slight variation in the accuracy (TPR) results were observed because of following reasons: 

We had slightly more data available than what original authors had. 

The authors found 70+ COVID related images after they completed their initial study, they had opportunity to revalidate their model using these new images. However, they did not mention how to find these images. As a result, we had to train our model with full dataset (including these 70+ images) and accuracy (TPR) of covid-19 class turned out to be better in the reproduction study than in original work. 

We performed random sampling of images for test-train split. This resulted in different train and test set between original work and reproduction work. 

\textbf{Ablation: } Evaluating models with f1-score provided us more insights into model performance. We found that unlike recall, f1-score consistently improved across all labels in the combined model. Therefore it further strengthened claim cited in paper.

\textbf{Strength and weakness: }We feel that strength of our work was that we were able to very closely mimic dataset used by the original authors in spite of code unavailability. The model training and validation code provided by authors cannot execute multiple models in parallel. This is because of stateful nature of code due to tensorflow v1. We could not fix it during our work due to time constraints. As a result our model training and validation time was slower than we expected. We consider it as one of the weaknesses in our approach. 

\subsection{What was easy}

Authors provided extensive documentation of data in the paper and the supplemental documents. This documentation included source of data, how they processed it and what was the outcome of processing (e.g., number of images after each processing step). 

The sources of data were easily accessible, as a result we did not have any issues with raw data acquisition. 

By following the documentation and result of data processing provided by authors, we were able to write our own scripts to generate final data needed for model training and validation. 

\subsection{What was difficult}

There were two items which we consider difficult in our reproduction study. 

Authors provided very little detail about the model architecture in the paper. We had to reverse engineer the code provided by them to understand the model architecture, its parameters and intermediate results. 

The code was broken, we had to fix it before we could get started. 

Authors used tensorflow v1, which is outdated now. We had to spend extra time to understand their code. Moreover it was extremely difficult to modify this code because it was not modular in nature. 

Original authors used tool CloDSA in the original work to augment images. Authors did not clearly mention how they did it. We had to learn this tool from scratch and implement our own scripts to perform augmentation activity.

\subsection{Recommendations for reproducibility}

Authors of this paper made a great effort to make their work reproducible. We would like to give them full credit for this. Following are the most important items that helped us to effectively reproduce the paper. The same are our recommendations for future authors as well.

\begin{itemize}
    \item Clearly mention source of data and its accessibility.
    \item Provide detailed steps about how data was pre-processed.
    \item If possible, provide summary of raw data, final data and intermediate data.
    \item If possible, provide source code. If not, then provide detailed model architecture and parameters.
\end{itemize}

\section{Communication with original authors}

Although we have utilized resources provided by authors meticulously to execute this reproduction study as close to the original work as possible. However we feel that we may have taken some assumptions along the way. To ensure that our report is a fair assessment of what authors intended to do, we sent our project draft to the authors and requested them to review it. We are hoping to hear back from them soon.

\clearpage

\bibliographystyle{acl_natbib}
\bibliography{acl2021}

\begin{enumerate}
    \item Cohen's covid-chestxray-dataset: \url{https://github.com/ieee8023/covid-chestxray-dataset}
    \item Kaggle NIH dataset: \url{https://www.kaggle.com/datasets/nih-chest-xrays/data}
    \item National Library of medicine: \url{https://lhncbc.nlm.nih.gov/LHC-publications/pubs/TuberculosisChestXrayImageDataSets.html}
    \item CovidPred: \url{https://github.com/arunsharma8osdd/covidpred}
    \item CloDSA: \url{https://github.com/joheras/CLoDSA}
    \item Reproduction work: \url{https://github.com/AshutoshAgarwal01/CovidPred_Repro}
    \item Original Paper: \url{https://www.hindawi.com/journals/ijbi/2020/8889023/}
    \item Accuracy, precision, recall: \url{https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826}
\end{enumerate}

%\appendix

\end{document}
