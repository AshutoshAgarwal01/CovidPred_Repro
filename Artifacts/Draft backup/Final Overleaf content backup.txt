%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper, varwidth=true, border=2pt]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{graphicx}
\usepackage{tabularx}
\graphicspath{ {./Artifact/} }
%\documentclass[varwidth=true, border=2pt]{standalone}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}

\usepackage[belowskip=-10pt,aboveskip=0pt]{caption}

\usepackage{xparse}
\NewDocumentCommand{\DIV}{om}{%
  \IfValueT{#1}{\setcounter{#2}{\numexpr#1-1\relax}}%
  \csname #2\endcsname
}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

% Content lightly modified from original work by Jesse Dodge and Noah Smith


\newcommand\BibTeX{B\textsc{ib}\TeX}
\title{CS598 DL4H Spring 2022 Project Draft}
\author{Ashutosh Agarwal and Chandan Goel \\
  \texttt{aa61@illinois.edu, chandan3@illinois.edu}
  \\[2em]
  Group ID: 179\\
  %% Paper ID: \href{https://www.hindawi.com/journals/ijbi/2020/8889023/}{External [7]}\\
  Paper ID: \href{https://www.hindawi.com/journals/ijbi/2020/8889023/}{External (Sharma, Rani \& Gupta 2020) [7]}\\
  Presentation link (TODO): \url{https://www.youtube.com} \\
  Code link: \url{https://github.com/AshutoshAgarwal01/CovidPred_Repro}} 

\begin{document}
\maketitle

% All sections are mandatory.
% Keep in mind that your page limit is 8, excluding references.
% For specific grading rubrics, please see the project instruction.

\section{Introduction}
This work \href{https://www.hindawi.com/journals/ijbi/2020/8889023/}{(Sharma, Rani \& Gupta 2020) [7]} is done to build efficient deep leaning models using the convolutional neural network (CNN) using a very limited set of chest X-ray images to differentiate COVID-19 cases from healthy cases and other types of illnesses. 

Several other studies were done before this work to classify COVID-19 patients using chest X-ray images. This study recognizes following issues in classification of COVID-19 based on chest X-ray that result in poor performance of models. 

\setlist{nolistsep}
\begin{itemize}[noitemsep]
\itemsep0em 
\item Scarcity of x-ray images available to train a viable deep learning model. 
\item Biased learning of the deep learning based model when images of multiple age groups are combined together to form a data set. E.g., x-ray images of pediatric patients combined with adult patients. 
\end{itemize}
 
In this study, authors have applied following techniques to overcome issues mentioned above: 

\begin{itemize}
\itemsep0em 
\item To overcome scarcity of x-ray images, authors proposed creating a much larger artificial dataset using smaller original dataset. This artificial dataset was created by applying 25 image augmentation techniques on the original data. 
\item Careful image selection: Authors propose to use similar type of images (same view, same age group) to train a very targeted model. This study claims that models built with similar type of images perform better than those models that take a wide variety of images. 
\end{itemize}

Overall, authors showed that artificially generated x-ray images using image augmentation techniques greatly improved model performance when compared with original smaller set of images. 

\section{Scope of reproducibility}

    \textit{"This study shows that a deep learning model trained with larger volume of artificially generated X-ray images using image augmentation techniques will have higher accuracy than the model trained with small set of original images."}

Data scarcity is a well-known problem in the field of healthcare analytics, especially when research is aimed at fairly new area e.g., classification of COVID-19 patients when the pandemic just broke out. We are motivated to take on this study as it tries to solve a real life problem which exists across all  healthcare domain. It will not only help improve model performance, but also reduce cost and time involved in getting sufficient size data for any healthcare study.  

\subsection{Addressed claims from the original paper}

We will be testing following two claims from the paper.
\begin{itemize}
    \item \textbf{Claim 1}: Model trained with original and augmented images combined results in higher accuracy than model trained with only original images across all classification labels.
    
    Authors compared performance of following two models on unseen external data. 

    \begin{itemize}
        \item Model with only original images. 
        \item Model with original images along with augmented images.
    \end{itemize}
    
    Authors concluded that model trained with original and augmented images combined, results in higher accuracy (True positive rate) across classification labels.  In our project work, we want to confirm this claim.
    
    \item \textbf{Claim 2}: Authors showed that models trained with 120 and 140 degree rotated images were complimentary to each other. i.e., if one model performed bad for certain label than other model would perform better for the same label. Together, these two models beat performance of model trained with original images. We will verify this claim in our reproduction study.
\end{itemize}

\section{Methodology}
\begin{figure*}[t]
  \includegraphics[width=\textwidth]{acl-ijcnlp2021-templates/Artifacts/CNN Architecture_v2.png}
  \caption{Model architecture}
  \label{fig:model-arch}
\end{figure*}

We did this work in three phases, namely i) image pre-processing, (ii) data augmentation, (iii) training of deep learning models. The above steps are explained in this section. We leveraged the authors code (\href{https://github.com/arunsharma8osdd/covidpred}{GitHub [4]}) with moderate modifications for the model training but used our own code for data processing.

%%The authors of the original paper made a diligent effort for anyone to understand their paper and code easily. They provided link to their GitHub repository \href{https://github.com/arunsharma8osdd/covidpred}{CovidPred} [4] where code was present and they clearly mentioned source of data that they used for their study.

%%We are planning to write new code for data processing and re-using (with moderate modifications) existing code for model training and testing. Following sections provide more details about this.

\subsection{Model descriptions}
The author used a simple CNN approach by testing a number of different architectures with different number of layers, number of neurons per layers, normalizations, pooling techniques and hyperparameters and adapting the model after each additional improvement of validation accuracy. The CNN model identified uses 3 convolutional layers followed by two fully connected linear layers. 

\textbf{Figure \ref{fig:model-arch}} - manifests the CNN model architecture developed for this paper. Rectified Linear Unit (ReLU) addressed the non- linearity after each convolutional layer.

%% , and dropout between the connected linear layers were used to standardize the inputs to a neural network and stabilize the validation loss along the epochs, reducing the variance of validation accuracy. 

The first convolutional layer has an input channel of dimension 3 since we are using RGB image of size 256 x 256. The kernel size is chosen to be of size 3x3 with stride of 1. The output dimension at this layer is 32 x 256 x256. This is followed by ReLu activation function to introduce non-linearity. To reduce the number of training parameters and computation cost in conjunction with controlling overfitting issue, a max pooling layer with kernel size 2x2 and stride 2 has been introduced after ReLU activation function. Max pooling layer down sample the output feature maps to 32 x 128 x 128. The second convolutional layer has an input channel of 32 to keep it consistent with the dimension of the feature map from the previous layer. The kernel size is chosen to be 5x5 with stride of 1. The output dimension at this layer is 64 x 128 x 128 so no changes occurred in the transformation channel dimension. Like previous channel ReLU activation function is applied followed by another max pooling layer with kernel size 2*2 with stride 2 is introduced to down sample the feature map led to increased efficiency in model training process. The down-sampled feature map dimension: 64 x 64 x 64.

The third convolutional layer is introduced to upgrade the output channel before feeding into linear layers. Like previous convolutional layers, subsequent maxpool, ReLU activation are applied to the output images. The output dimension at this layer is 128 x 32 x 32. Finally, two fully connected linear layers are used at the end. A flattened version of the feature map is passed to the first fully connected layer. So, the input dimension size has become 128 x 32 x 32 = 131072
nodes. Then this layer is connected to the final linear layer. The output dimension of final layer should match the total image corresponds to five categories: Normal, TB, Pneumonia, Covid-19, Non Covid-19.

\subsection{Data descriptions}
Data from following three sources is used in this paper.

\begin{itemize}
    \item Github (Cohen's covid-chestxray-dataset) [1]: This dataset is used to get 'covid 19' and 'non covid 19' images 
    \item Kaggle NIH dataset [2]: This dataset is used to get Pneumonia images. 
    \item National Library of medicine [3]: This dataset is used to get normal and TB images. 
\end{itemize}

\textbf{Figure \ref{fig:data-processing}} depicts all steps we performed to gather and process data for model training and validation purposes.
\begin{figure}
\includegraphics[scale=0.45]{acl-ijcnlp2021-templates/Artifacts/Data_Processing.png}
\caption{Number in each box is count of images.}
\label{fig:data-processing}
\end{figure}
%% \textit {Note: Number in each box represents number of images in respective dataset.}

We filtered images from these datasets using following criteria: i). Age of patient must be 19 years or older, ii) only chest X-ray images and iii) image view must be PA

After filtering the data, we divided the dataset into two parts using random sampling. We reserved 10 percent of the data for external validation (test set) and remaining 90 percent for model training (training set).

After this, we created 25 new datasets by applying different augmentation techniques on original set of training and test images. Some of the augmentation techniques used are:
\begin{itemize}
    \item Rotate images by 45, 60, 90, 120, 140 and 160 degrees
    \item Raise blue, green, red and hue
    \item Crop images
    \item Flip images horizontally, vertically and in both directions.
    \item Introduce blur to the images.
\end{itemize}

We used CloDSA [5] library for image augmentation.

We further created one more dataset by combining original dataset and all augmented datasets. Thus we had total 27 datasets.

%% Following table shows distribution of images.
%% \begin{figure}
%% \includegraphics[scale=0.4]{acl-ijcnlp2021-templates/Artifacts/Data_Distribution.png}
%% \end{figure}

\subsection{Hyperparameters}

For initial reproduction work, we used same hyperparameters used in the paper except number of iterations. Table in \textbf{Figure \ref{fig:hyperparameters}} describes the hyperparameters and their values across different datasets. We used AdamOptimizer with learning rate of 1e-4.

\begin{figure}
\includegraphics[scale=0.61]{acl-ijcnlp2021-templates/Artifacts/Hyperparameters.png}
\caption{Hyperparameters}
\label{fig:hyperparameters}
\end{figure}

\subsection{Implementation}
We thoroughly studied the code available in author's git repository [4]. Following is distribution of code that we wrote and re-used.

\begin{itemize}
    \item Data processing: We wrote our own code from scratch for data processing. 
    \item Model training and validation: We made some improvements in existing code like logging, refactoring, minor performance improvements and training/ validating models using multiple datasets by using one script etc. But overall, it is heavily inspired by original work.
\end{itemize}

Code of reproduction work can be found in Git repository CovidPred\_Repo [6]
%% \url{https://github.com/AshutoshAgarwal01/CovidPred_Repro}

\subsection{Computational requirements}
In the proposal we mentioned that we will use GPU if training model with large dataset (all augmented images combined) takes very long time. However in our initial runs, even the largest dataset completed in reasonable amount of time. Therefore we decided to use the alternate in-premise CPU based hardware. Tables (\textbf{Figure \ref{fig:configuration}} and \textbf{Figure \ref{fig:resource-utilization}}) show configuration of desktop that we used and total resources consumed during work.
\begin{figure}
\includegraphics[scale=0.9]{acl-ijcnlp2021-templates/Artifacts/DesktopConfig.png}
\caption{Configuration of hardware}
\label{fig:configuration}
\end{figure}
\begin{figure}
\includegraphics[scale=0.8]{acl-ijcnlp2021-templates/Artifacts/ResourcesConsumed.png}
\caption{Resource utilization}
\label{fig:resource-utilization}
\end{figure}

Table in \textbf{Figure \ref{fig:cputime}} summarizes total CPU time and average CPU time per epoch for each model.
\begin{figure}
\includegraphics[scale=0.5]{acl-ijcnlp2021-templates/Artifacts/Training_Time.png}
\caption{CPU time spent for training}
\label{fig:cputime}
\end{figure}

%% Total size of original, filtered, augmented and combined data together was \textbf{119 GB} on disk.

\section{Results}
Our reproduction study did not result in exact same accuracy numbers as the original study. However overall trend of the accuracy (true positive) per label is consistent with the original study.

%%The results obtained by our reproduction study support both the claims cited in section 2 of this paper. We will describe them in following sections.

% The number of subsections for results should be the same as the number of hypotheses you are trying to verify.

\subsection{Result 1: (Results/ Analyses/ Plans)}

%\subsubsection{Result 1}


The paper claimed that model trained with original and augmented images combined results in higher accuracy (true positive) across all classification labels when compared with model trained with original images only.

%Table in \textit{figure \ref{fig:orig-comb-perf}} summarizes true positive rate (proportions of images correctly classified by the model for given label) of both models on unseen original data.

Table in \textbf{Figure \ref{fig:orig-comb-perf}} summarizes true positive rate (proportions of images correctly classified by the model for given label) of both models on unseen original data.
We can see that model trained with combined dataset outperforms model trained with original images by over 20\% on average. This upholds the paper's conclusion that overall it performs better than the baseline. 

We observed that the combined model performs 100\% better on Normal, over 42\% better on Covid-19, over 8\% better on Pneumonia and 25\% better on TB illnesses. However, accuracy for non-covid degrades in combined model. As per our analysis, we feel it happened due to a smaller test dataset size of 3 images. 

We plan to introduce more non-covid19 images to our model in final study to confirm our analysis. In addtion, we also plan simulate the study multiple times to further analyze the outcome for a more accurate comparison.


\begin{figure}
\includegraphics[scale=0.67]{acl-ijcnlp2021-templates/Artifacts/Perf_comp_with_combined.png}
\caption{Original vs combined performance}
\label{fig:orig-comb-perf}
\end{figure}

\subsection{Result 2: (Results/ Analyses/ Plans)}
Authors showed that performance of models trained with 120 and 140 degree rotated images were complimentary to each other. Together, these two models beat performance of model trained with original images. 

Table in \textbf{Figure \ref{fig:orig-aug-perf}} summarizes true positive rate (proportions of images correctly classified by the model for given label) for all three models on unseen original data.

We observed that the two models perform with 100\% accuracy on Normal images. Model with 120 degree performs better in detecting Covid-19 while Model with 140 degree performs better on other illnesses. However, both augmented models perform worse than baseline for TB. Overall, the result trend with exception to TB is in line with the original study. Therefore, this claim is only partially supported by our reproduction study. 

We found that the difference is due to random sampling of TB images when compared with original study. We plan to further analyze and test the model with additional random sampling and compare the average result to study the trend. We also plan to simulate the study with new data sets for comparison.

%% As per our analysis we found that the difference is because of random sampling of TB images when compared with original study. We plan to further analyze and test the model with additional random sampling and compare the average result to study the trend.

\begin{figure}
\includegraphics[scale=0.7]{acl-ijcnlp2021-templates/Artifacts/Perf_comp_with_augmented.png}
\caption{Original vs rotated comparison}
\label{fig:orig-aug-perf}
\end{figure}

\subsection{Additional results not present in the original paper: TODO}

\section{Discussion: TODO}

\subsection{What was easy}

Authors provided extensive documentation of data in the paper and the supplemental documents. This documentation included source of data, how they processed it and what was the outcome of processing (e.g., number of images after each processing step). 

The sources of data were easily accessible, as a result we did not have any issues with raw data acquisition. 

By following the documentation and result of data processing provided by authors, we were able to write our own scripts to generate final data needed for model training and validation. 

\subsection{What was difficult}

There were two items which we consider difficult in our reproduction study. 

Authors did not provide any detail about the model architecture in the paper or in their supplementary material. We had to reverse engineer the code provided by them to understand the model architecture, its parameters and intermediate results. 

The code was broken, we had to fix it before we could get started. 

Authors used tensorflow v1, which is outdated now. We had to spend extra time to understand their code. Moreover it was extremely difficult to modify this code because it was not modular in nature. 

Original authors used tool CloDSA in the original work to augment images. Authors did not clearly mention how they did it. We had to learn this tool from scratch and implement our own scripts to perform augmentation activity.

\subsection{Recommendations for reproducibility: TODO}

\section{Communication with original authors: TODO}

We did not have any specific questions to ask about the paper itself. However, we sent our project draft to the author (Dinesh Gupta) and requested him to review the same. We have not heard back from him so far.

\clearpage

\bibliographystyle{acl_natbib}
\bibliography{acl2021}

\begin{enumerate}
    \item Cohen's covid-chestxray-dataset: \url{https://github.com/ieee8023/covid-chestxray-dataset}
    \item Kaggle NIH dataset: \url{https://www.kaggle.com/datasets/nih-chest-xrays/data}
    \item National Library of medicine: \url{https://lhncbc.nlm.nih.gov/LHC-publications/pubs/TuberculosisChestXrayImageDataSets.html}
    \item CovidPred: \url{https://github.com/arunsharma8osdd/covidpred}
    \item CloDSA: \url{https://github.com/joheras/CLoDSA}
    \item Reproduction work: \url{https://github.com/AshutoshAgarwal01/CovidPred_Repro}
    \item Original Paper: \url{https://www.hindawi.com/journals/ijbi/2020/8889023/}
\end{enumerate}

%\appendix

\end{document}
